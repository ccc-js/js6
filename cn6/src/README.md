# nn1 -- 深度學習背後的算法

用程式示範《反傳遞演算法》的原理！

```
$ cd ex2-backprop

$ node .\gradientEx.js
forward: f()
{ x: Node { v: 2, g: 0 },
  y: Node { v: 1, g: 0 },
  x2: Node { v: 4, g: 0 },
  y2: Node { v: 1, g: 0 },
  o: Node { v: 5, g: 0 } }
backward: grad()
{ x: Node { v: 2, g: 4 },
  y: Node { v: 1, g: 1 },
  x2: Node { v: 4, g: 1 },
  y2: Node { v: 1, g: 1 },
  o: Node { v: 5, g: 1 } }

$ node backPropOptimizeEx

p= {x:2.0000, y:1.0000} f(p)= 5
  gp= { x: 4, y: 1 }
p= {x:1.9600, y:0.9900} f(p)= 4.8217
  gp= { x: 3.8415999999999997, y: 0.9801 }
p= {x:1.9216, y:0.9802} f(p)= 4.653275148657
  gp= { x: 3.692485069056, y: 0.960790079601 }
p= {x:1.8847, y:0.9706} f(p)= 4.493987190929792
  gp= { x: 3.5519401090757823, y: 0.9420470818540095 }
// ... 中間省略 ....
p= {x:0.0479, y:0.0468} f(p)= 0.004479629245409157
  gp= { x: 0.0022920829015963253, y: 0.0021875463438128314 }
p= {x:0.0479, y:0.0467} f(p)= 0.004475389263812065
  gp= { x: 0.00228988872537307, y: 0.0021855005384389947 }
p= {x:0.0478, y:0.0467} f(p)= 0.004471155300865204
  gp= { x: 0.002287697698821976, y: 0.0021834576020432284 }
p= {x:0.0478, y:0.0467} f(p)= 0.004466927345179781
  gp= { x: 0.002285509815916863, y: 0.002181417529262918 }
```

